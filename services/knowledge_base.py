import json
import os
import requests
from typing import List, Dict, Any
import faiss
import numpy as np
from config import Config

# å°è¯•å¯¼å…¥sentence_transformersï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: sentence_transformers ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…")

class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self):
        self.config = Config()
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.document_embeddings = []
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹...")
                print("ğŸ“¦ æ¨¡å‹åç§°: paraphrase-multilingual-MiniLM-L12-v2")
                print("â³ è¯·ç¨å€™ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
                
                # æ·»åŠ è¿›åº¦æç¤º
                import time
                start_time = time.time()
                
                # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ç¼“å­˜
                import os
                cache_dir = os.path.join(os.getcwd(), 'model_cache')
                os.makedirs(cache_dir, exist_ok=True)
                os.environ['TRANSFORMERS_CACHE'] = cache_dir
                os.environ['HF_HOME'] = cache_dir
                os.environ['HF_HUB_OFFLINE'] = '1'  # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
                
                # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹
                local_model_path = "~/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d"
                
                if os.path.exists(local_model_path):
                    print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
                    try:
                        self.embedding_model = SentenceTransformer(local_model_path)
                        end_time = time.time()
                        load_time = end_time - start_time
                        print(f"âœ… æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f}ç§’")
                    except Exception as local_error:
                        print(f"âš ï¸ æœ¬åœ°è·¯å¾„åŠ è½½å¤±è´¥: {local_error}")
                        print("å°è¯•ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½...")
                        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                        end_time = time.time()
                        load_time = end_time - start_time
                        print(f"âœ… æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f}ç§’")
                else:
                    print("âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç½‘ç»œåŠ è½½...")
                    os.environ['HF_HUB_OFFLINE'] = '0'  # å…è®¸ç½‘ç»œè¿æ¥
                    self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                    end_time = time.time()
                    load_time = end_time - start_time
                    print(f"âœ… æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {load_time:.2f}ç§’")
                    
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("å°†ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…æ¨¡å¼")
                self.embedding_model = None
        else:
            print("âš ï¸ sentence_transformers ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…")
            self.embedding_model = None
        
    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“æ•°æ®"""
        print("ğŸ“š å¼€å§‹åŠ è½½çŸ¥è¯†åº“...")
        knowledge_path = self.config.KNOWLEDGE_BASE_PATH
        
        # åŠ è½½FAQçŸ¥è¯†åº“
        faq_path = os.path.join(knowledge_path, "product_faq.json")
        if os.path.exists(faq_path):
            print("ğŸ“– åŠ è½½FAQçŸ¥è¯†åº“...")
            with open(faq_path, 'r', encoding='utf-8') as f:
                faq_data = json.load(f)
                self._process_faq_data(faq_data)
            print(f"âœ… FAQçŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…± {len(faq_data.get('faqs', []))} ä¸ªé—®é¢˜")
        else:
            print("âš ï¸ FAQçŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½å•†å“åˆ†ç±»çŸ¥è¯†åº“
        category_path = os.path.join(knowledge_path, "product_categories.json")
        if os.path.exists(category_path):
            print("ğŸ·ï¸ åŠ è½½å•†å“åˆ†ç±»çŸ¥è¯†åº“...")
            with open(category_path, 'r', encoding='utf-8') as f:
                category_data = json.load(f)
                self._process_category_data(category_data)
            print(f"âœ… å•†å“åˆ†ç±»çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œå…± {len(category_data.get('categories', []))} ä¸ªåˆ†ç±»")
        else:
            print("âš ï¸ å•†å“åˆ†ç±»çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ„å»ºå‘é‡ç´¢å¼•
        print("ğŸ” æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")
        self._build_vector_index()
        print("âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼")
    
    def _process_faq_data(self, faq_data: Dict[str, Any]):
        """å¤„ç†FAQæ•°æ®"""
        for faq in faq_data.get('faqs', []):
            # åˆ›å»ºæ–‡æ¡£å†…å®¹
            content = f"é—®é¢˜ï¼š{faq['question']}\nç­”æ¡ˆï¼š{faq['answer']}\nåˆ†ç±»ï¼š{faq['category']}"
            
            # æ·»åŠ å…³é”®è¯
            if 'keywords' in faq:
                content += f"\nå…³é”®è¯ï¼š{', '.join(faq['keywords'])}"
            
            self.documents.append({
                'content': content,
                'type': 'faq',
                'category': faq.get('category', ''),
                'question': faq['question'],
                'answer': faq['answer']
            })
    
    def _process_category_data(self, category_data: Dict[str, Any]):
        """å¤„ç†å•†å“åˆ†ç±»æ•°æ®"""
        for category in category_data.get('categories', []):
            category_name = category['name']
            
            for subcategory in category.get('subcategories', []):
                subcategory_name = subcategory['name']
                keywords = subcategory.get('keywords', [])
                common_questions = subcategory.get('common_questions', [])
                
                # åˆ›å»ºåˆ†ç±»æ–‡æ¡£
                content = f"å•†å“åˆ†ç±»ï¼š{category_name} - {subcategory_name}\n"
                content += f"ç›¸å…³å•†å“ï¼š{', '.join(keywords)}\n"
                content += f"å¸¸è§é—®é¢˜ï¼š{', '.join(common_questions)}"
                
                self.documents.append({
                    'content': content,
                    'type': 'category',
                    'category': category_name,
                    'subcategory': subcategory_name,
                    'keywords': keywords,
                    'common_questions': common_questions
                })
    
    def _build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if not self.documents:
            return
        
        # å¦‚æœæ²¡æœ‰embeddingæ¨¡å‹ï¼Œä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        if self.embedding_model is None:
            print("ä½¿ç”¨ç®€å•å…³é”®è¯åŒ¹é…æ¨¡å¼")
            return
        
        try:
            # æå–æ–‡æ¡£å†…å®¹
            texts = [doc['content'] for doc in self.documents]
            print(f"ğŸ“ å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£...")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            print("ğŸ§  ç”Ÿæˆæ–‡æœ¬å‘é‡...")
            embeddings = self.embedding_model.encode(texts, convert_to_tensor=False)
            self.document_embeddings = embeddings
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼ç”Ÿæˆ {embeddings.shape[0]} ä¸ªå‘é‡ï¼Œç»´åº¦: {embeddings.shape[1]}")
            
            # æš‚æ—¶è·³è¿‡FAISSç´¢å¼•ï¼Œç›´æ¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
            print("ğŸ” ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å¼")
        except Exception as e:
            print(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨ç®€å•å…³é”®è¯åŒ¹é…æ¨¡å¼")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³çŸ¥è¯†"""
        if not self.documents:
            return []
        
        # å¦‚æœæ²¡æœ‰å‘é‡ç´¢å¼•ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
        if self.index is None or self.embedding_model is None:
            return self._keyword_search(query, top_k)
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query], convert_to_tensor=False)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            results = []
            for i, doc in enumerate(self.documents):
                if self.document_embeddings is not None and i < len(self.document_embeddings):
                    doc_embedding = self.document_embeddings[i]
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding[0], doc_embedding) / (
                        np.linalg.norm(query_embedding[0]) * np.linalg.norm(doc_embedding)
                    )
                    result = doc.copy()
                    result['similarity_score'] = float(similarity)
                    results.append(result)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_k
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            return results[:top_k]
        except Exception as e:
            print(f"å‘é‡æœç´¢å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…: {e}")
            return self._keyword_search(query, top_k)
    
    def _keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ç®€å•çš„å…³é”®è¯æœç´¢"""
        query_lower = query.lower()
        results = []
        
        for doc in self.documents:
            content_lower = doc['content'].lower()
            score = 0
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            for word in query_lower.split():
                if word in content_lower:
                    score += 1
            
            if score > 0:
                result = doc.copy()
                result['similarity_score'] = score / len(query.split())
                results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k
        results.sort(key=lambda x: x['similarity_score'], reverse=True)
        return results[:top_k]
    
    def get_context_for_query(self, query: str, max_context_length: int = 1000) -> str:
        """è·å–æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        search_results = self.search(query, top_k=3)
        
        context_parts = []
        current_length = 0
        
        for result in search_results:
            if result['type'] == 'faq':
                context_part = f"FAQ - {result['question']}: {result['answer']}"
            else:
                context_part = f"å•†å“åˆ†ç±»ä¿¡æ¯: {result['content']}"
            
            if current_length + len(context_part) <= max_context_length:
                context_parts.append(context_part)
                current_length += len(context_part)
            else:
                break
        
        return "\n\n".join(context_parts)
    
    def add_custom_knowledge(self, content: str, knowledge_type: str = "custom", **kwargs):
        """æ·»åŠ è‡ªå®šä¹‰çŸ¥è¯†"""
        document = {
            'content': content,
            'type': knowledge_type,
            **kwargs
        }
        
        self.documents.append(document)
        
        # é‡æ–°æ„å»ºç´¢å¼•
        self._build_vector_index()
    
    def download_external_knowledge(self):
        """ä¸‹è½½å¤–éƒ¨çŸ¥è¯†åº“"""
        for url in self.config.TAOBAO_KNOWLEDGE_URLS:
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    # æ ¹æ®æ•°æ®ç»“æ„å¤„ç†
                    if 'faqs' in data:
                        self._process_faq_data(data)
                    elif 'categories' in data:
                        self._process_category_data(data)
            except Exception as e:
                print(f"ä¸‹è½½çŸ¥è¯†åº“å¤±è´¥ {url}: {e}")
    
    def save_knowledge_base(self, filepath: str):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°æ–‡ä»¶"""
        data = {
            'documents': self.documents,
            'embeddings': self.document_embeddings if self.document_embeddings is not None else None
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def load_knowledge_base_from_file(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“"""
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            self.documents = data.get('documents', [])
            embeddings_data = data.get('embeddings')
            
            if embeddings_data:
                self.document_embeddings = np.array(embeddings_data)
                self._build_vector_index() 